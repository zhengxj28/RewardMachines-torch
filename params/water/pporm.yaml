testing_params:
  test: true
  test_freq: 10000  #10000
  num_steps: 600

learning_params:
  total_units: 200
  step_unit: 10000  # 10000
  lr: 0.00001  # 5e-5 seems to be better than 1e-4
  epsilon: 0.1
  gamma: 0.9
  buffer_size: 50000
  train_freq: 1
  batch_size: 64
  target_network_update_freq: 100  # obs: 500 makes learning more stable, but slower
  learning_starts: 1
  tabular_case: false
  use_random_maps: false

  clip_rate: 0.1
  lam: 0.8
  n_updates: 10
  policy_loss_coef: 1.0
  value_loss_coef: 1.0
  entropy_loss_coef: 0.01


model_params:
  tabular_case: false
  num_hidden_layers: 6
  num_neurons: 64

