testing_params:
  test: true
  test_freq: 10000  #10000
  num_steps: 600

learning_params:
  total_units: 100
  step_unit: 1000  # 1000
  lr: 0.00001  # 5e-5 seems to be better than 1e-4
  epsilon: 0.1
  gamma: 0.9
  buffer_size: 50000
  train_freq: 1
  batch_size: 32
  target_network_update_freq: 100  # obs: 500 makes learning more stable, but slower
  learning_starts: 1000
  tabular_case: false
  use_random_maps: false
  use_double_dqn: true
  prioritized_replay: true
  max_ltl_len: 25

model_params:
  tabular_case: false
  num_hidden_layers: 6
  num_neurons: 64

  # transformer params
  d_model: 64
  nhead: 8
  num_encoder_layers: 4
  pool: 'mean'
  dim_feedforward: 256
  dropout: 0.0
  d_out: 16
  layer_norm_eps: 0.00001
  TFixup: true


