testing_params:
  test: true
  test_freq: 10000  #10000
  num_steps: 600

learning_params:
  total_units: 500
  step_unit: 10000  # 10000
  lr: 0.0001  # 5e-5 seems to be better than 1e-4
  epsilon: 0.1
  gamma: 0.9
  buffer_size: 50000
  train_freq: 1
  batch_size: 32
  target_network_update_freq: 100  # obs: 500 makes learning more stable, but slower
  learning_starts: 1000
  tabular_case: false
  use_random_maps: false
  use_double_dqn: true
  prioritized_replay: true
  max_ltl_len: 25

model_params:
  tabular_case: false
  num_hidden_layers: 6
  num_neurons: 256

  type: "embedding" # "transformer" or "embedding"
  enc_feature: "concat" # "concat" or "fine-tune"

  # transformer params
  d_model: 64
  nhead: 8
  num_encoder_layers: 4
  d_out: 16 # output_dim of the encoding module
  pool: 'mean'
  dim_feedforward: 256
  dropout: 0.0
  layer_norm_eps: 0.00001
  TFixup: true

  # embedding params
  max_num_formulas: 40  # max num of LTL formulas
  embedding_dim: 32
